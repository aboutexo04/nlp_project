{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# ì…€ 3: ë°ì´í„° ë¡œë“œ\n",
    "DATA_PATH = \"/content/data/\" # Corrected typo from '/condent/data/'\n",
    "RESULT_PATH = \"/content/prediction/\"\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "val_df = pd.read_csv(os.path.join(DATA_PATH, 'dev.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dec709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# ì…€ 4: ì¹´í…Œê³ ë¦¬ë³„ Few-shot ìƒ˜í”Œ ì„¤ì •\n",
    "\n",
    "# í† í”½ì„ ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ê·¸ë£¹í•‘í•˜ëŠ” í•¨ìˆ˜\n",
    "def categorize_topic(topic):\n",
    "    topic = topic.lower() if isinstance(topic, str) else \"\"\n",
    "\n",
    "    # ìŒì‹/ì£¼ë¬¸ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ìŒì‹', 'ì£¼ë¬¸', 'ë°°ë‹¬', 'ì‹ë‹¹', 'ë©”ë‰´', 'ì‹ì‚¬', 'ìš”ë¦¬', 'ì¹´í˜', 'ì»¤í”¼']):\n",
    "        return 'food_order'\n",
    "\n",
    "    # ë©´ì ‘/ì·¨ì—… ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ë©´ì ‘', 'ì·¨ì—…', 'ì±„ìš©', 'ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œ']):\n",
    "        return 'interview'\n",
    "\n",
    "    # ì—¬í–‰/ì˜ˆì•½ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì—¬í–‰', 'í˜¸í…”', 'ì˜ˆì•½', 'í•­ê³µ', 'ìˆ™ì†Œ', 'ì²´í¬ì¸', 'ê´€ê´‘', 'íœ´ê°€']):\n",
    "        return 'travel'\n",
    "\n",
    "    # ì‡¼í•‘/ê±°ë˜ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì‡¼í•‘', 'êµ¬ë§¤', 'ê°€ê²©', 'í• ì¸', 'í™˜ë¶ˆ', 'ìƒí’ˆ', 'ì„ëŒ€']):\n",
    "        return 'shopping'\n",
    "\n",
    "    # ì˜ë£Œ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì˜ë£Œ', 'ë³‘ì›', 'ì§„ë£Œ', 'ì¦ìƒ', 'ê±´ê°•', 'ì•½']):\n",
    "        return 'medical'\n",
    "\n",
    "    # ê¸¸/êµí†µ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ê¸¸', 'íƒì‹œ', 'êµí†µ', 'ë²„ìŠ¤', 'ì§€í•˜ì² ', 'ìš´ì „', 'ì£¼í–‰']):\n",
    "        return 'transport'\n",
    "\n",
    "    # ì¼ìƒ/ê¸°íƒ€\n",
    "    return 'daily'\n",
    "\n",
    "# train_dfì— category ì»¬ëŸ¼ ì¶”ê°€\n",
    "train_df['category'] = train_df['topic'].apply(categorize_topic)\n",
    "val_df['category'] = val_df['topic'].apply(categorize_topic)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ few-shot ìƒ˜í”Œ ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "category_samples = {}\n",
    "for cat in train_df['category'].unique():\n",
    "    cat_df = train_df[train_df['category'] == cat]\n",
    "    samples = cat_df.sample(min(2, len(cat_df)), random_state=42)\n",
    "    category_samples[cat] = {\n",
    "        'dialogue1': samples.iloc[0]['dialogue'],\n",
    "        'summary1': samples.iloc[0]['summary'],\n",
    "        'dialogue2': samples.iloc[1]['dialogue'] if len(samples) > 1 else samples.iloc[0]['dialogue'],\n",
    "        'summary2': samples.iloc[1]['summary'] if len(samples) > 1 else samples.iloc[0]['summary'],\n",
    "    }\n",
    "\n",
    "print(\"ì¹´í…Œê³ ë¦¬ë³„ few-shot ìƒ˜í”Œ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ì¹´í…Œê³ ë¦¬ ëª©ë¡: {list(category_samples.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71117027",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- train_df head ---')\n",
    "display(train_df.head())\n",
    "print('\\n--- val_df head ---')\n",
    "display(val_df.head())\n",
    "\n",
    "print('\\n--- train_df info ---')\n",
    "train_df.info()\n",
    "print('\\n--- val_df info ---')\n",
    "val_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10aa395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í”½ ë¶„í¬ ì „ì²´ í™•ì¸\n",
    "print(\"=\" * 50)\n",
    "print(\"í† í”½ ë¶„í¬ (ìƒìœ„ 20ê°œ)\")\n",
    "print(\"=\" * 50)\n",
    "topic_counts = train_df['topic'].value_counts()\n",
    "print(topic_counts.head(20))\n",
    "\n",
    "print(f\"\\nì´ ê³ ìœ  í† í”½ ìˆ˜: {train_df['topic'].nunique()}\")\n",
    "\n",
    "# í† í”½ë³„ ìƒ˜í”Œ ìˆ˜ ë¶„í¬\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"í† í”½ë³„ ìƒ˜í”Œ ìˆ˜ í†µê³„\")\n",
    "print(\"=\" * 50)\n",
    "print(topic_counts.describe())\n",
    "\n",
    "# ìƒ˜í”Œ ìˆ˜ ì ì€ í† í”½ë“¤\n",
    "print(\"\\nìƒ˜í”Œ ìˆ˜ 10ê°œ ì´í•˜ í† í”½:\")\n",
    "print(topic_counts[topic_counts <= 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49da7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í”½ì„ ìƒìœ„ ì¹´í…Œê³ ë¦¬ë¡œ ê·¸ë£¹í•‘\n",
    "def categorize_topic(topic):\n",
    "    topic = topic.lower() if isinstance(topic, str) else \"\"\n",
    "\n",
    "    # ìŒì‹/ì£¼ë¬¸ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ìŒì‹', 'ì£¼ë¬¸', 'ë°°ë‹¬', 'ì‹ë‹¹', 'ë©”ë‰´', 'ì‹ì‚¬', 'ìš”ë¦¬', 'ì¹´í˜', 'ì»¤í”¼']):\n",
    "        return 'food_order'\n",
    "\n",
    "    # ë©´ì ‘/ì·¨ì—… ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ë©´ì ‘', 'ì·¨ì—…', 'ì±„ìš©', 'ì´ë ¥ì„œ', 'ìê¸°ì†Œê°œ']):\n",
    "        return 'interview'\n",
    "\n",
    "    # ì—¬í–‰/ì˜ˆì•½ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì—¬í–‰', 'í˜¸í…”', 'ì˜ˆì•½', 'í•­ê³µ', 'ìˆ™ì†Œ', 'ì²´í¬ì¸', 'ê´€ê´‘', 'íœ´ê°€']):\n",
    "        return 'travel'\n",
    "\n",
    "    # ì‡¼í•‘/ê±°ë˜ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì‡¼í•‘', 'êµ¬ë§¤', 'ê°€ê²©', 'í• ì¸', 'í™˜ë¶ˆ', 'ìƒí’ˆ', 'ì„ëŒ€']):\n",
    "        return 'shopping'\n",
    "\n",
    "    # ì˜ë£Œ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ì˜ë£Œ', 'ë³‘ì›', 'ì§„ë£Œ', 'ì¦ìƒ', 'ê±´ê°•', 'ì•½']):\n",
    "        return 'medical'\n",
    "\n",
    "    # ê¸¸/êµí†µ ê´€ë ¨\n",
    "    if any(kw in topic for kw in ['ê¸¸', 'íƒì‹œ', 'êµí†µ', 'ë²„ìŠ¤', 'ì§€í•˜ì² ', 'ìš´ì „', 'ì£¼í–‰']):\n",
    "        return 'transport'\n",
    "\n",
    "    # ì¼ìƒ/ê¸°íƒ€\n",
    "    return 'daily'\n",
    "\n",
    "train_df['category'] = train_df['topic'].apply(categorize_topic)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬\")\n",
    "print(\"=\" * 50)\n",
    "print(train_df['category'].value_counts())\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ ì˜ˆì‹œ í† í”½ í™•ì¸\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ì¹´í…Œê³ ë¦¬ë³„ í† í”½ ì˜ˆì‹œ\")\n",
    "print(\"=\" * 50)\n",
    "for cat in train_df['category'].unique():\n",
    "    topics = train_df[train_df['category'] == cat]['topic'].value_counts().head(5)\n",
    "    print(f\"\\n[{cat}]\")\n",
    "    print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ í™•ì¸ (dialogue, summary êµ¬ì¡° íŒŒì•…)\n",
    "print(\"=\" * 50)\n",
    "print(\"ë°ì´í„° ì»¬ëŸ¼ í™•ì¸\")\n",
    "print(\"=\" * 50)\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# ì²« ë²ˆì§¸ ìƒ˜í”Œë¡œ dialogue/summary í•„ë“œëª… í™•ì¸\n",
    "print(\"\\nì²« ë²ˆì§¸ ìƒ˜í”Œ í‚¤:\")\n",
    "print(train_df.iloc[0].to_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc504912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í…Œê³ ë¦¬ë³„ ìƒ˜í”Œ ì˜ˆì‹œ í™•ì¸ (dialogue, summary ê¸¸ì´ í¬í•¨)\n",
    "# í•„ë“œëª…ì€ ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
    "\n",
    "dialogue_col = 'dialogue'  # ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìˆ˜ì •\n",
    "summary_col = 'summary'    # ì‹¤ì œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ìˆ˜ì •\n",
    "\n",
    "for cat in ['food_order', 'interview', 'travel', 'shopping', 'medical', 'transport', 'daily']:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"[{cat}] ìƒ˜í”Œ ì˜ˆì‹œ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    sample = train_df[train_df['category'] == cat].iloc[0]\n",
    "\n",
    "    print(f\"\\ní† í”½: {sample['topic']}\")\n",
    "    print(f\"\\nëŒ€í™”:\\n{sample[dialogue_col][:500]}...\")\n",
    "    print(f\"\\nìš”ì•½:\\n{sample[summary_col]}\")\n",
    "    print(f\"\\nëŒ€í™” ê¸¸ì´: {len(sample[dialogue_col])}, ìš”ì•½ ê¸¸ì´: {len(sample[summary_col])}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9548e5",
   "metadata": {},
   "source": [
    "### 2. ê²°ì¸¡ì¹˜ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- train_df missing values ---')\n",
    "print(train_df.isnull().sum())\n",
    "print('\\n--- val_df missing values ---')\n",
    "print(val_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db25b99",
   "metadata": {},
   "source": [
    "### 3. ëŒ€í™” ë° ìš”ì•½ ê¸¸ì´ ë¶„í¬ ë¶„ì„\n",
    "\n",
    "ëŒ€í™”ì™€ ìš”ì•½ë¬¸ì˜ ë‹¨ì–´ ë° ê¸€ì ìˆ˜ ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„°ì˜ íŠ¹ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ëŒ€í™” ê¸¸ì´ (ê¸€ì ìˆ˜)\n",
    "train_df['dialogue_len_char'] = train_df['dialogue'].apply(len)\n",
    "val_df['dialogue_len_char'] = val_df['dialogue'].apply(len)\n",
    "\n",
    "# ìš”ì•½ ê¸¸ì´ (ê¸€ì ìˆ˜)\n",
    "train_df['summary_len_char'] = train_df['summary'].apply(len)\n",
    "val_df['summary_len_char'] = val_df['summary'].apply(len)\n",
    "\n",
    "# ëŒ€í™” ê¸¸ì´ (ë‹¨ì–´ ìˆ˜)\n",
    "train_df['dialogue_len_word'] = train_df['dialogue'].apply(lambda x: len(x.split()))\n",
    "val_df['dialogue_len_word'] = val_df['dialogue'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# ìš”ì•½ ê¸¸ì´ (ë‹¨ì–´ ìˆ˜)\n",
    "train_df['summary_len_word'] = train_df['summary'].apply(lambda x: len(x.split()))\n",
    "val_df['summary_len_word'] = val_df['summary'].apply(lambda x: len(x.split()))\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(train_df['dialogue_len_char'], bins=50, kde=True)\n",
    "plt.title('Train Dialogue Length (Characters)')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(train_df['summary_len_char'], bins=50, kde=True)\n",
    "plt.title('Train Summary Length (Characters)')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(train_df['dialogue_len_word'], bins=50, kde=True)\n",
    "plt.title('Train Dialogue Length (Words)')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(train_df['summary_len_word'], bins=50, kde=True)\n",
    "plt.title('Train Summary Length (Words)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n--- Train Dialogue Length (Characters) Statistics ---')\n",
    "print(train_df['dialogue_len_char'].describe())\n",
    "print('\\n--- Train Summary Length (Characters) Statistics ---')\n",
    "print(train_df['summary_len_char'].describe())\n",
    "print('\\n--- Train Dialogue Length (Words) Statistics ---')\n",
    "print(train_df['dialogue_len_word'].describe())\n",
    "print('\\n--- Train Summary Length (Words) Statistics ---')\n",
    "print(train_df['summary_len_word'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2630226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON íŒŒì¼ ëª©ë¡ (9ê°œ):\n",
      "  - ìƒê±°ë˜(ì‡¼í•‘).json\n",
      "  - ì‹ìŒë£Œ.json\n",
      "  - ë¯¸ìš©ê³¼ê±´ê°•.json\n",
      "  - ì—¬ê°€ìƒí™œ.json\n",
      "  - ì‹œì‚¬êµìœ¡.json\n",
      "  - ì¼ê³¼ì§ì—….json\n",
      "  - ê°œì¸ë°ê´€ê³„.json\n",
      "  - í–‰ì‚¬.json\n",
      "  - ì£¼ê±°ì™€ìƒí™œ.json\n",
      "\n",
      "==================================================\n",
      "\n",
      "ğŸ“ ìƒê±°ë˜(ì‡¼í•‘).json\n",
      "  íƒ€ì…: <class 'dict'>\n",
      "  í‚¤: ['numberOfItems', 'data']\n",
      "  numberOfItems: 1666\n",
      "  data ê°œìˆ˜: 1666ê°œ\n",
      "\n",
      "  ì²« ë²ˆì§¸ ìƒ˜í”Œ:\n",
      "{\n",
      "  \"header\": {\n",
      "    \"dialogueInfo\": {\n",
      "      \"dialogueID\": \"34e45d83-41ea-5321-a983-bc257da2848f\",\n",
      "      \"numberOfParticipants\": 2,\n",
      "      \"numberOfUtterances\": 16,\n",
      "      \"numberOfTurns\": 8,\n",
      "      \"type\": \"ì¼ìƒ ëŒ€í™”\",\n",
      "      \"topic\": \"ìƒê±°ë˜(ì‡¼í•‘)\"\n",
      "    },\n",
      "    \"participantsInfo\": [\n",
      "      {\n",
      "        \"participantID\": \"P01\",\n",
      "        \"gender\": \"ì—¬ì„±\",\n",
      "        \"age\": \"20ëŒ€\",\n",
      "        \"residentialProvince\": \"ì „ë¼ë‚¨ë„\"\n",
      "      },\n",
      "      {\n",
      "        \"participantID\": \"P02\",\n",
      "        \"gender\": \"ì—¬ì„±\",\n",
      "        \"age\": \"20ëŒ€\",\n",
      "        \"residentialProvince\": \"ì „ë¼ë‚¨ë„\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"body\": {\n",
      "    \"dialogue\": [\n",
      "      {\n",
      "        \"utteranceID\": \"U1\",\n",
      "        \"turnID\": \"T1\",\n",
      "        \"participantID\": \"P01\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"12:52:00\",\n",
      "        \"utterance\": \"íƒë°°ëŠ” ì˜¬ ìƒê°ì„ ì•ˆí•œë‹¤\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U2\",\n",
      "        \"turnID\": \"T2\",\n",
      "        \"participantID\": \"P02\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"12:54:00\",\n",
      "        \"utterance\": \"ë„ˆë„ë‹ˆ\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U3\",\n",
      "        \"turnID\": \"T2\",\n",
      "        \"participantID\": \"P02\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"12:55:00\",\n",
      "        \"utterance\": \"ë‚´íƒë°°ë„ì˜¬ìƒê°ì„ì•ˆí•œë‹¤\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U4\",\n",
      "        \"turnID\": \"T3\",\n",
      "        \"participantID\": \"P01\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"13:14:00\",\n",
      "        \"utterance\": \"ì§œì¦ë‚˜ã… ã…œã… ã…œã… ã…œã… \"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U5\",\n",
      "        \"turnID\": \"T3\",\n",
      "        \"participantID\": \"P01\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"13:14:00\",\n",
      "        \"utterance\": \"ì–¼ë¥¸ ì™”ìœ¼ë©´ ì¢‹ê² ì—Œã…‹ã…‹\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U6\",\n",
      "        \"turnID\": \"T4\",\n",
      "        \"participantID\": \"P02\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"13:15:00\",\n",
      "        \"utterance\": \"ë‚˜ë§Œí•˜ê² ë‹ˆ\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U7\",\n",
      "        \"turnID\": \"T4\",\n",
      "        \"participantID\": \"P02\",\n",
      "        \"date\": \"2015-04-02\",\n",
      "        \"time\": \"13:15:00\",\n",
      "        \"utterance\": \"ã…‹ã…‹ã…‹ã…‹\"\n",
      "      },\n",
      "      {\n",
      "        \"utteranceID\": \"U8\",\n",
      "        \"turnID\": \"T5\",\n",
      "        \"particip\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# AI Hub JSON ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# JSON íŒŒì¼ ê²½ë¡œ\n",
    "data_dir = '/Users/seoyeonmun/nlp_project/korean_data/train_15000'\n",
    "\n",
    "# í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ í™•ì¸\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "print(f\"JSON íŒŒì¼ ëª©ë¡ ({len(json_files)}ê°œ):\")\n",
    "for f in json_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# ê° JSON íŒŒì¼ì˜ êµ¬ì¡° í™•ì¸\n",
    "for json_file in json_files:\n",
    "    json_path = os.path.join(data_dir, json_file)\n",
    "    \n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"\\nğŸ“ {json_file}\")\n",
    "    print(f\"  íƒ€ì…: {type(data)}\")\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        print(f\"  ê°œìˆ˜: {len(data)}ê°œ\")\n",
    "        print(f\"\\n  ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "        print(json.dumps(data[0], indent=2, ensure_ascii=False)[:1000])\n",
    "    \n",
    "    elif isinstance(data, dict):\n",
    "        print(f\"  í‚¤: {list(data.keys())}\")\n",
    "        \n",
    "        # 'data' í‚¤ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë‚´ìš© í™•ì¸\n",
    "        if 'data' in data:\n",
    "            print(f\"  numberOfItems: {data.get('numberOfItems', 'N/A')}\")\n",
    "            print(f\"  data ê°œìˆ˜: {len(data['data'])}ê°œ\")\n",
    "            print(f\"\\n  ì²« ë²ˆì§¸ ìƒ˜í”Œ:\")\n",
    "            print(json.dumps(data['data'][0], indent=2, ensure_ascii=False)[:2000])\n",
    "        else:\n",
    "            first_key = list(data.keys())[0]\n",
    "            print(f\"\\n  ì²« ë²ˆì§¸ í‚¤({first_key}) ë‚´ìš©:\")\n",
    "            content = data[first_key][:1] if isinstance(data[first_key], list) else data[first_key]\n",
    "            print(json.dumps(content, indent=2, ensure_ascii=False)[:2000])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    break  # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ ìƒì„¸ í™•ì¸ (ì „ì²´ ë³´ë ¤ë©´ ì´ ì¤„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f706de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
