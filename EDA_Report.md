# 한국어 대화 요약 데이터셋 EDA 리포트

## 1. 데이터셋 개요

### 기본 정보
- **데이터셋 크기**: 279,992개
- **데이터 소스**: AI Hub 한국어 대화 요약 데이터
- **컬럼**: `dialogue`, `summary`
- **결측치**: 없음
- **파일 개수**: 9개 JSON 파일

### 데이터 카테고리
- 개인및관계
- 미용과건강
- 상거래(쇼핑)
- 시사교육
- 식음료
- 여가생활
- 일과직업
- 주거와생활
- 행사

---

## 2. 길이 분석

### 2.1 대화 길이 통계
| 통계량 | 값 |
|--------|------|
| 평균 | 149.4 글자 |
| 중앙값 | 130.0 글자 |
| 최소 | 7 글자 |
| 최대 | 8,653 글자 |
| 표준편차 | 93.4 |

### 2.2 요약 길이 통계
| 통계량 | 값 |
|--------|------|
| 평균 | 36.4 글자 |
| 중앙값 | 34.0 글자 |
| 최소 | 2 글자 |
| 최대 | 189 글자 |
| 표준편차 | 13.9 |

### 2.3 압축 비율
- **평균 압축 비율**: 24.39%
- 대화가 요약으로 약 1/4 크기로 압축됨

---

## 3. 데이터 분포

### 3.1 대화 길이 구간별 분포
| 구간 | 개수 | 비율 |
|------|------|------|
| 0-100 글자 | 93,931개 | 33.5% |
| 100-200 글자 | 157,538개 | 56.3% |
| 200-300 글자 | 23,529개 | 8.4% |
| 300-500 글자 | 4,611개 | 1.6% |
| 500-1000 글자 | 377개 | 0.1% |
| 1000+ 글자 | 6개 | 0.002% |

**인사이트**:
- 대부분의 대화(89.8%)가 200글자 이하
- 100-200 글자 구간이 가장 많음 (56.3%)

### 3.2 요약 길이 구간별 분포
| 구간 | 개수 | 비율 |
|------|------|------|
| 0-50 글자 | 207,622개 | 74.1% |
| 50-100 글자 | 72,267개 | 25.8% |
| 100-150 글자 | 101개 | 0.04% |
| 150-200 글자 | 2개 | 0.001% |

**인사이트**:
- 대부분의 요약(99.9%)이 100글자 이하
- 74%가 50글자 이하의 짧은 요약

---

## 4. 상관관계 분석

### 대화 길이 vs 요약 길이
- **상관계수**: 0.576
- 중간 정도의 양의 상관관계
- 대화가 길어질수록 요약도 대체로 길어지는 경향

---

## 5. 이상치 분석

### 5.1 대화 길이 이상치
- **개수**: 11,722개 (4.19%)
- IQR 기준 1.5배를 벗어나는 데이터
- 주로 매우 긴 대화들

### 5.2 요약 길이 이상치
- **개수**: 1,441개 (0.51%)
- 대부분 요약이 일정한 길이 범위 내에 있음

---

## 6. 샘플 데이터 분석

### 샘플 1: 여행 계획
- **대화 길이**: 224 글자
- **요약 길이**: 36 글자
- **압축 비율**: 16.1%
- **내용**: 비행기 표 가격 및 특가 이벤트 대기

### 샘플 2: 마스크 구매
- **대화 길이**: 182 글자
- **요약 길이**: 35 글자
- **압축 비율**: 19.2%
- **내용**: 비염 때문에 일회용 부직포 마스크 구매

### 샘플 3: 케이크 주문
- **대화 길이**: 177 글자
- **요약 길이**: 72 글자
- **압축 비율**: 40.7%
- **내용**: 케이크 업체 비교 및 배달 조건 논의

---

## 7. 주요 인사이트

### 7.1 데이터 품질
✅ **장점**:
- 결측치 없음
- 28만개의 대규모 데이터셋
- 다양한 일상 주제 포괄
- 일정한 요약 길이 유지

⚠️ **주의사항**:
- 4.19%의 대화 길이 이상치 존재
- 극단적으로 긴 대화 (최대 8,653자)
- 극단적으로 짧은 요약 (최소 2자)

### 7.2 모델링 고려사항

1. **토큰 길이 설정**
   - 대화: 평균 149.4자 → 약 200-300 토큰
   - 요약: 평균 36.4자 → 약 50-80 토큰
   - max_length: 대화 512, 요약 128 권장

2. **데이터 전처리**
   - 극단적 길이의 데이터 필터링 고려
   - 1000자 이상 대화: 제거 또는 분할
   - 100자 이상 요약: 검토 필요

3. **배치 크기 최적화**
   - 대부분 짧은 대화 → 배치 처리 효율적
   - Dynamic padding 활용 권장

4. **평가 지표**
   - ROUGE-1: 유니그램 기반 재현율/정밀도
   - ROUGE-2: 바이그램 기반 재현율/정밀도
   - ROUGE-L: 최장 공통 부분 수열 기반

---

## 8. 다음 단계 권장사항

### 8.1 추가 분석
- [ ] 토픽별 길이 분포 차이 분석
- [ ] 대화 턴 수와 요약 품질 관계
- [ ] 특수문자, 이모티콘 사용 패턴
- [ ] 중복 데이터 검사

### 8.2 데이터 전처리
- [ ] 이상치 처리 전략 수립
- [ ] Train/Validation/Test 분할 (8:1:1)
- [ ] 토큰화 및 길이 제한
- [ ] 데이터 증강 검토

### 8.3 모델 실험
- [ ] Baseline 모델: KoBART, KoT5
- [ ] Fine-tuning 전략
- [ ] 하이퍼파라미터 튜닝
- [ ] 앙상블 모델 검토

---

## 9. 시각화 요약

생성된 그래프:
1. **대화 길이 분포 히스토그램**: 좌편향 분포, 100-200 구간 집중
2. **요약 길이 분포 히스토그램**: 정규분포에 가까움, 30-40 구간 집중
3. **압축 비율 분포**: 평균 24.4%, 10-40% 범위
4. **산점도**: 대화-요약 길이 상관관계 (r=0.576)
5. **박스플롯**: 대화 길이 이상치 많음, 요약은 안정적

---

## 10. 결론

이 데이터셋은 **한국어 대화 요약 모델 학습에 적합**합니다:

### 강점
- 충분한 데이터 크기 (28만개)
- 다양한 일상 주제
- 깨끗한 데이터 (결측치 없음)
- 일정한 압축 비율 (24%)

### 개선 필요
- 극단적 길이 데이터 처리
- 이상치 필터링 전략
- 요약 품질 수동 검증 (샘플링)

**권장 모델**: KoBART, KoT5, mBART 등 한국어 사전학습 Seq2Seq 모델

---

*리포트 생성일: 2025-11-14*
*분석 도구: Python, Pandas, Matplotlib, Seaborn*
